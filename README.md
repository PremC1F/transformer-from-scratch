# Transformer from Scratch

This project implements a transformer architecture from scratch using PyTorch's basic tensor operations. The implementation closely follows the architecture described in the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al.

## Features

- Complete transformer architecture with encoder and decoder
- Multi-head self-attention and cross-attention mechanisms
- Positional encodings and embeddings
- Visualization tools for attention patterns
- Example applications for text classification and machine translation
- Comprehensive test suite

## Project Structure