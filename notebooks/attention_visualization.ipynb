{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing Transformer Attention Patterns\n",
        "\n",
        "This notebook demonstrates how to visualize attention patterns in our custom transformer implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from src.transformer import Transformer\n",
        "from src.utils import visualize_attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a Toy Example\n",
        "\n",
        "Let's create a small transformer model and generate some attention patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a small transformer model\n",
        "vocab_size = 1000\n",
        "embed_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "ff_dim = 128\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=vocab_size,\n",
        "    tgt_vocab_size=vocab_size,\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        "    ff_dim=ff_dim\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a toy input sequence\n",
        "batch_size = 1\n",
        "seq_len = 10\n",
        "\n",
        "src = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
        "tgt = torch.randint(1, vocab_size, (batch_size, seq_len))\n",
        "\n",
        "# Create masks\n",
        "src_mask, tgt_mask, src_tgt_mask = Transformer.create_masks(src, tgt)\n",
        "\n",
        "# Forward pass\n",
        "output, attention_maps = model(src, tgt, src_mask, tgt_mask, src_tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Encoder Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get encoder attention maps (from the last layer, first head)\n",
        "encoder_attention = attention_maps['encoder_attention'][-1][0, 0].detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(encoder_attention, annot=True, cmap='viridis')\n",
        "plt.title(\"Encoder Self-Attention (Layer 2, Head 1)\")\n",
        "plt.xlabel(\"Key Position\")\n",
        "plt.ylabel(\"Query Position\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Decoder Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get decoder self-attention maps (from the last layer, first head)\n",
        "decoder_self_attention = attention_maps['decoder_self_attention'][-1][0, 0].detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(decoder_self_attention, annot=True, cmap='viridis')\n",
        "plt.title(\"Decoder Self-Attention (Layer 2, Head 1)\")\n",
        "plt.xlabel(\"Key Position\")\n",
        "plt.ylabel(\"Query Position\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Decoder Cross-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get decoder cross-attention maps (from the last layer, first head)\n",
        "decoder_cross_attention = attention_maps['decoder_cross_attention'][-1][0, 0].detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(decoder_cross_attention, annot=True, cmap='viridis')\n",
        "plt.title(\"Decoder Cross-Attention (Layer 2, Head 1)\")\n",
        "plt.xlabel(\"Encoder Key Position\")\n",
        "plt.ylabel(\"Decoder Query Position\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention Pattern Analysis\n",
        "\n",
        "Let's analyze what patterns emerge in the attention weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics for each attention type\n",
        "for name, attention_type in [\n",
        "    (\"Encoder Self-Attention\", encoder_attention),\n",
        "    (\"Decoder Self-Attention\", decoder_self_attention),\n",
        "    (\"Decoder Cross-Attention\", decoder_cross_attention)\n",
        "]:\n",
        "    print(f\"\\n{name} Statistics:\")\n",
        "    print(f\"Mean: {attention_type.mean():.4f}\")\n",
        "    print(f\"Max: {attention_type.max():.4f}\")\n",
        "    print(f\"Min: {attention_type.min():.4f}\")\n",
        "    print(f\"Standard Deviation: {attention_type.std():.4f}\")\n",
        "    \n",
        "    # Check for diagonal dominance in self-attention\n",
        "    if \"Self\" in name:\n",
        "        diagonal = np.diag(attention_type)\n",
        "        off_diagonal = attention_type[~np.eye(attention_type.shape[0], dtype=bool)]\n",
        "        print(f\"Diagonal Mean: {diagonal.mean():.4f}\")\n",
        "        print(f\"Off-Diagonal Mean: {off_diagonal.mean():.4f}\")\n",
        "        print(f\"Diagonal/Off-Diagonal Ratio: {diagonal.mean() / off_diagonal.mean():.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
